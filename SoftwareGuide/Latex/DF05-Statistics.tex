\chapter{Statistics}
\label{sec:StaisticsFramework}

This chapter introduces the statistics functionalities in Insight. The
statistics subsystem's primary purpose is to provide general capabilities
for statistical pattern classification. However, its use is not limited
for classification. Users might want to use data containers and
algorithms in the statistics subsystem to perform other statistical
analysis or to preprocess image data for other tasks.

The statistics subsystem mainly consists of three parts: data container
classes, statistical algorithms, and the classification framework. In this
chapter, we will discuss each major part in that order.

\section{Data Containers}
\label{sec:StatisticsDataContainer}

An \subdoxygen{Statistics}{Sample} object is a data container of elements
that we call \emph{measurement vectors}. A measurement vector is an array of
values (of the same type) measured on an object (In images, it can be a
vector of the gray intensity value and/or the gradient value of a
pixel). Strictly speaking from the design of the Sample class, a measurement
vector can be any class derived from \doxygen{FixedArray}, including
FixedArray itself.

\begin{figure}
  \centering
  \includegraphics[width=0.9\textwidth]{SampleInheritanceTree.eps}
  \itkcaption[Sample class inheritance tree]{Sample class inheritance diagram.}
  \protect\label{fig:SampleInheritanceTree}
\end{figure}

\subsection{Sample Interface}
\label{sec:SampleInterface}

\ifitkFullVersion
\input{ListSample.tex}
\fi

\subsection{Sample Adaptors}
\label{sec:SampleAdaptors}

There are two adaptor classes that provide the common
\subdoxygen{Statistics}{Sample} interfaces for \doxygen{Image} and
\doxygen{PointSet}, two fundamental data container classes found in ITK. The
adaptor classes do not store any real data elements themselves. These data
come from the source data container plugged into them. First, we will
describe how to create an
\subdoxygen{Statistics}{ImageToListSampleAdaptor} and then an
\subdoxygen{Statistics}{PointSetToListSampleAdaptor} object.

\subsubsection{ImageToListSampleAdaptor}
\label{sec:ImageToListSampleAdaptor}

\ifitkFullVersion
\input{ImageToListSampleAdaptor.tex}
\fi

\subsubsection{PointSetToListSampleAdaptor}
\label{sec:PointSetToListSampleAdaptor}

\ifitkFullVersion
\input{PointSetToListSampleAdaptor.tex}
\fi

\ifitkFullVersion
\input{PointSetToAdaptor.tex}
\fi


\subsection{Histogram}
\label{sec:Histogram}

\ifitkFullVersion
\input{Histogram.tex}
\fi

\subsection{Subsample}
\label{sec:Subsample}

\ifitkFullVersion
\input{Subsample.tex}
\fi

\subsection{MembershipSample}
\label{sec:MembershipSample}

\ifitkFullVersion
\input{MembershipSample.tex}
\fi

\subsection{MembershipSampleGenerator}
\label{sec:MembershipSampleGenerator}

\ifitkFullVersion
\input{MembershipSampleGenerator.tex}
\fi


\subsection{K-d Tree}
\label{sec:KdTree}

\ifitkFullVersion
\input{KdTree.tex}
\fi

\section{Algorithms and Functions}
\label{sec:StatisticsAlgorithmsFunctions}

In the previous section, we described the data containers in the ITK
statistics subsystem. We also need data processing algorithms and statistical
functions to conduct statistical analysis or statistical classification using
these containers. Here we define an algorithm to be an operation over a set
of measurement vectors in a sample. A function is an operation over
individual measurement vectors. For example, if we implement a class
(\subdoxygen{Statistics}{EuclideanDistance}) to calculate the Euclidean
distance between two measurement vectors, we call it a function, while if we
implemented a class (\subdoxygen{Statistics}{MeanCalculator}) to calculate
the mean of a sample, we call it an algorithm.

\subsection{Sample Statistics}
\label{sec:SampleStatistics}

We will show how to get sample statistics such as means and covariance from
the (\subdoxygen{Statistics}{Sample}) classes. Statistics can tells us
characteristics of a sample. Such sample statistics are very important for
statistical classification. When we know the form of the sample distributions
and their parameters (statistics), we can conduct Bayesian classification. In
ITK, sample mean and covariance calculation algorithms are implemented. Each
algorithm also has its weighted version (see Section
\ref{sec:WeightedMeanCovariance}). The weighted versions are used in the
expectation-maximization parameter estimation process.

\subsubsection{Mean and Covariance}
\label{sec:MeanCovariance}

\ifitkFullVersion
\input{SampleStatistics.tex}
\fi

\subsubsection{Weighted Mean and Covariance}
\label{sec:WeightedMeanCovariance}

\ifitkFullVersion
\input{WeightedSampleStatistics.tex}
\fi

\subsection{Sample Generation}
\label{sec:SampleGeneration}

\subsubsection{SampleToHistogramFilter}
\label{sec:SampleToHistogramFilter}

\ifitkFullVersion
\input{SampleToHistogramFilter.tex}
\fi

% TODO HACK Determine where this code went
%REMOVED \subsubsection{ListSampleToHistogramGenerator}
%REMOVED \label{sec:ListSampleToHistogramGenerator}
%REMOVED
%REMOVED \ifitkFullVersion
%REMOVED \input{ListSampleToHistogramGenerator.tex}
%REMOVED \fi

\subsubsection{NeighborhoodSampler}
\label{sec:NeighborhoodSampler}

\ifitkFullVersion
\input{NeighborhoodSampler.tex}
\fi

% TODO HACK Determine where this code went
%REMOVED \subsubsection{SampleToHistogramProjectionFilter}
%REMOVED \label{sec:SampleToHistogramProjectionFilter}
%REMOVED
%REMOVED \ifitkFullVersion
%REMOVED \input{SampleToHistogramProjectionFilter.tex}
%REMOVED \fi

\subsection{Sample Sorting}
\label{sec:SampleSorting}

\ifitkFullVersion
\input{SampleSorting.tex}
\fi

\subsection{Probability Density Functions}
\label{sec:ProbabilityDensityFunctions}

The probability density function (PDF) for a specific distribution returns
the probability density for a measurement vector. To get the probability
density from a PDF, we use the \code{Evaluate(input)} method. PDFs for
different distributions require different sets of distribution
parameters. Before calling the \code{Evaluate()} method, make sure to set the
proper values for the distribution parameters.

\subsubsection{Gaussian Distribution}
\label{sec:GaussianMembershipFunction}

\ifitkFullVersion
\input{GaussianMembershipFunction.tex}
\fi

\subsection{Distance Metric}
\label{sec:DistanceMetric}

\subsubsection{Euclidean Distance}
\label{sec:EuclideanDistanceMetric}

\ifitkFullVersion
\input{EuclideanDistanceMetric.tex}
\fi

\subsection{Decision Rules}
\label{sec:DecisionRules}

A decision rule is a function that returns the index of one data element in a
vector of data elements. The index returned depends on the internal logic of
each decision rule. The decision rule is an essential part of the ITK
statistical classification framework. The scores from a set of membership
functions (e.g. probability density functions, distance metrics) are compared
by a decision rule and a class label is assigned based on the output of the
decision rule. The common interface is very simple. Any decision rule class
must implement the \code{Evaluate()} method. In addition to this method,
certain decision rule class can have additional method that accepts prior
knowledge about the decision task. The
\doxygen{MaximumRatioDecisionRule} is an example of such a class.

The argument type for the \code{Evaluate()} method is
\code{std::vector< double >}. The decision rule classes are part of the
\code{itk} namespace instead of \code{itk::Statistics} namespace.

For a project that uses a decision rule, it must link the \code{itkCommon}
library. Decision rules are not templated classes.

\subsubsection{Maximum Decision Rule}
\label{sec:MaximumDecisionRule}

\ifitkFullVersion
\input{MaximumDecisionRule.tex}
\fi

\subsubsection{Minimum Decision Rule}
\label{sec:MinimumDecisionRule}

\ifitkFullVersion
\input{MinimumDecisionRule.tex}
\fi

\subsubsection{Maximum Ratio Decision Rule}
\label{sec:MaximumRatioDecisionRule}

\input{MaximumRatioDecisionRule.tex}

\subsection{Random Variable Generation}
\label{sec:RandomVariableGeneration}

A random variable generation class returns a variate when the
\code{GetVariate()} method is called. When we repeatedly call the method
for ``enough'' times, the set of variates we will get follows
the distribution form of the random variable generation class.

\subsubsection{Normal (Gaussian) Distribution}
\label{sec:NormalVariateGeneration}

\ifitkFullVersion
\input{NormalVariateGenerator.tex}
\fi


\section{Statistics applied to Images}
\label{sec:StatisticsAppliedToImages}

\subsection{Image Histograms}
\label{sec:ImageHistogram}


\subsubsection{Scalar Image Histogram with Adaptor}
\label{sec:ScalarImageHistogramAdaptor}
\ifitkFullVersion
\input{ImageHistogram1.tex}
\fi


\subsubsection{Scalar Image Histogram with Generator}
\label{sec:ScalarImageHistogramGenerator}
\ifitkFullVersion
\input{ImageHistogram2.tex}
\fi


\subsubsection{Color Image Histogram with Generator}
\label{sec:ColorImageHistogramGenerator}
\ifitkFullVersion
\input{ImageHistogram3.tex}
\fi


\subsubsection{Color Image Histogram Writing}
\label{sec:ColorImageHistogramGeneratorWriting}
\ifitkFullVersion
\input{ImageHistogram4.tex}
\fi


\subsection{Image Information Theory}
\label{sec:ImageInformationTheory}

Many concepts from Information Theory have been used successfully in the domain
of image processing. This section introduces some of such concepts and
illustrates how the statistical framework in ITK can be used for computing
measures that have some relevance in terms of Information Theory
\cite{Shannon1948,Shannon1949,Kullback1997}.


\subsubsection{Computing Image Entropy}
\label{sec:ComputingImageEntropy}

\index{Entropy!What's wrong in images}

The concept of Entropy has been introduced into image processing as a crude
mapping from its application in Communications. The notions of Information
Theory can be deceiving and misleading when applied to images because their
language from Communication Theory does not necessarily map to what people in
the Imaging Community use.

For example, it is commonly said that

\emph{``The Entropy of an image is a measure of the amount of information
contained in an image''}.

This statement is fundamentally \textbf{incorrect}.

The way the notion of Entropy is commonly measured in images is by first
assuming that the spatial location of a pixel in an image is irrelevant!  That
is, we simply take the statistical distribution of the pixel values as it can
be evaluated in a histogram and from that histogram we estimate the frequency
of the value associated to each bin. In other words, we simply assume that the
image is a set of pixels that are passing through a channel, just as things are
commonly considered for communication purposes.

Once the frequency of every pixel value has been estimated, Information Theory
defines that the amount of uncertainty that an observer will lose by taking one
pixel and finding its real value to be the one associated with the i-th bin of the
histogram, is given by $-\log_2{(p_i)}$, where $p_i$ is the frequency in that
histogram bin. Since a reduction in uncertainty is equivalent to an increase in
the amount of information in the observer, we conclude that measuring one pixel
and finding its level to be in the i-th bin results in an acquisition of
$-\log_2{(p_i)}$ bits of information\footnote{Note that \textbf{bit} is the unit of
amount of information. Our modern culture has vulgarized the bit and its
multiples, the Byte, KiloByte, MegaByte, GigaByte and so on as simple measures
of the amount of RAM memory and capacity of a hard drive in a computer. In that
sense, a confusion is created between the encoding of a piece of data and its
actual amount of information. For example a file composed of one million
letters will take one million bytes in a hard disk, but it does not necessarily
have one million bytes of information, since in many cases parts of the file can
be predicted from others. This is the reason why data compression can manage to
compact files.}.

Since we could have picked any pixel at random, our chances of picking the ones
that are associated to the i-th histogram bin are given by $p_i$. Therefore,
the expected reduction in uncertainty that we can get from measuring the value
of one pixel is given by

\begin{equation}
H = - \sum_i{ p_i  \cdot \log_2{(p_i)} }
\end{equation}

This quantity $H$ is what is usually defined as the \emph{Entropy of the
Image}. It would be more accurate to call it the Entropy of the random variable
associated to the intensity value of \emph{one} pixel. The fact that $H$ is
unrelated to the spatial arrangement of the pixels in an image shows how little
of the real \emph{Image Information} $H$ actually represents. The Entropy
of an image, as measured above, is only a crude indication of how the intensity
values are spread in the dynamic range of intensities. For example, an image
with maximum entropy will be the one that has a large dynamic range and every
value in that range is equally probable.

The common acceptance of $H$ as a representation of image information has
terribly undermined the enormous potential on the application of Information
Theory to image processing and analysis.

The real concepts of Information Theory would require that we define the amount
of information in an image based on our expectations and prior knowledge from
that image. In particular, the \emph{Amount of Information} provided by an
image should measure the number of features that we are not able to predict
based on our prior knowledge about that image. For example, if we know that we
are going to analyze a CT scan of the abdomen of an adult human male in the age
range of 40 to 45, there is already a good deal that we could predict about the
content of that image.  The real amount of information in the image is the
representation of the features in the image that we could not predict from
knowing that it is a CT scan from a human adult male.

The application of Information Theory to image analysis is still in its early
infancy and it is an exciting and promising field to be explored further. All
that being said, let's now look closer at how the concept of Entropy (which is
not the amount of information in an image) can be measured with the ITK
statistics framework.

\ifitkFullVersion
\input{ImageEntropy1.tex}
\fi

\subsubsection{Computing Images Mutual Information}
\label{sec:ComputingImagesMutualInformation}

\ifitkFullVersion
\input{ImageMutualInformation1.tex}
\fi



\section{Classification}
\label{sec:Classification}

In statistical classification, each object is represented by $d$ features (a
measurement vector), and the goal of classification becomes finding compact and
disjoint regions (decision regions\cite{Duda2000}) for classes in a
$d$-dimensional feature space. Such decision regions are defined by decision
rules that are known or can be trained.  The simplest configuration of a
classification consists of a decision rule and multiple membership functions;
each membership function represents a class. Figure~\ref{fig:simple}
illustrates this general framework.

\begin{figure}[h]
  \centering
  \includegraphics[width=0.7\textwidth]{DudaClassifier.eps}
  \itkcaption[Simple conceptual classifier]{Simple conceptual classifier.}
  \label{fig:simple}
\end{figure}

This framework closely follows that of Duda and
Hart\cite{Duda2000}. The classification process can be described
as follows:

\begin{enumerate}
\item{A measurement vector is input to each membership function.}
\item{Membership functions feed the membership scores to the
    decision rule.}
\item{A decision rule compares the membership scores and returns a
    class label.}
\end{enumerate}

\begin{figure}
  \centering
  \includegraphics[width=0.7\textwidth]{StatisticalClassificationFramework.eps}
  \itkcaption[Statistical classification framework]{Statistical classification
framework.}
  \protect\label{fig:StatisticalClassificationFramework}
\end{figure}

This simple configuration can be used to formulated various classification
tasks by using different membership functions and incorporating task specific
requirements and prior knowledge into the decision rule. For example, instead
of using probability density functions as membership functions, through
distance functions and a minimum value decision rule (which assigns a class
from the distance function that returns the smallest value) users can achieve a
least squared error classifier. As another example, users can add a rejection
scheme to the decision rule so that even in a situation where the membership
scores suggest a ``winner'', a measurement vector can be flagged as ill-defined.
Such a rejection scheme can avoid risks of assigning a class label
without a proper win margin.

\subsection{k-d Tree Based k-Means Clustering}
\label{sec:KdTreeBasedKMeansClustering}
\ifitkFullVersion
\input{KdTreeBasedKMeansClustering.tex}
\fi

\subsection{K-Means Classification}
\label{sec:KMeansClassifier}
\ifitkFullVersion
\input{ScalarImageKmeansClassifier.tex}
\fi

\subsection{Bayesian Plug-In Classifier}
\label{sec:BayesianPluginClassifier}

\ifitkFullVersion
\input{BayesianPluginClassifier.tex}
\fi


\subsection{Expectation Maximization Mixture Model Estimation}
\label{sec:ExpectationMaximizationMixtureModelEstimation}

\ifitkFullVersion
\input{ExpectationMaximizationMixtureModelEstimator.tex}
\fi

\subsection{Classification using Markov Random Field}
\label{sec:MarkovRandomField}

Markov Random Fields are probabilistic models that use the correlation between
pixels in a neighborhood to decide the object region. The
\subdoxygen{Statistics}{MRFImageFilter} uses the maximum a posteriori (MAP)
estimates for modeling the MRF. The object traverses the data set and uses the
model generated by the Mahalanobis distance classifier to gets the the distance
between each pixel in the data set to a set of known classes, updates the
distances by evaluating the influence of its neighboring pixels (based on a MRF
model) and finally, classifies each pixel to the class which has the minimum
distance to that pixel (taking the neighborhood influence under consideration).
The energy function minimization is done using the iterated conditional modes
(ICM) algorithm \cite{Besag1986}.

\ifitkFullVersion
\input{ScalarImageMarkovRandomField1.tex}
\fi
